# ğŸŒ Distributed DistilGPT-2

**Multi-instance distributed LLM inference** with layer-wise sharding across GPU instances.

## âœ¨ Features

- ğŸš€ **GPU-accelerated** inference on Tesla T4/L4 
- ğŸ”„ **Layer sharding** - split model across instances
- ğŸŒ **Multi-node** - true distributed serving
- âš¡ **FastAPI** - REST API with auto-docs
- ğŸ§ª **Autoregressive** generation with sampling

## ğŸ—ï¸ Architecture

```
Instance 1 (Shard 0)    Instance 2 (Shard 1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layers 0-2     â”‚â”€â”€â”€â–¶â”‚  Layers 3-5     â”‚
â”‚  (Input Shard)  â”‚    â”‚  (Output Shard) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                        â”‚
        â”‚        Client          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1ï¸âƒ£ Setup Both Instances

```bash
# Clone repo
git clone <your-repo-url>
cd llm_p2p

# Setup environment (installs NVIDIA drivers if needed)
./setup_env.sh

# Reboot if NVIDIA drivers were installed
sudo reboot
```

### 2ï¸âƒ£ Get Instance IPs

```bash
# Get your public IP
curl -s http://checkip.amazonaws.com/
```

### 3ï¸âƒ£ Start Shards

**Instance 1** (Input Shard):
```bash
./setup_shard1.sh <instance1_ip> <instance2_ip>
```

**Instance 2** (Output Shard):  
```bash
./setup_shard2.sh <instance1_ip> <instance2_ip>
```

### 4ï¸âƒ£ Start Client

**On either instance**:
```bash
./setup_client.sh <instance1_ip> <instance2_ip>
```

### 5ï¸âƒ£ Test

```bash
curl -X POST http://localhost:9000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "The future of AI is", "max_length": 30}'
```

## ğŸ§ª Local Testing

Test multi-instance on single machine:
```bash
./deploy_multi_node.sh demo
```

## ğŸ“Š API Endpoints

- **Health**: `GET /health` - Check system status
- **Generate**: `POST /generate` - Text generation
- **Docs**: `GET /docs` - Interactive API docs
- **Shards**: `GET /shards` - Shard information

## âš™ï¸ Configuration

Each script auto-generates configs with your IPs:
- `shard1_config.json` - Shard 0 configuration  
- `shard2_config.json` - Shard 1 configuration
- `client_config.json` - Client coordination

## ğŸ”§ Requirements

- **GPU**: NVIDIA T4/L4 with 16GB+ VRAM
- **OS**: Ubuntu 20.04+ with NVIDIA drivers
- **Network**: Open port 8000 (shards) and 9000 (client)
- **Memory**: 8GB+ RAM per instance

## ğŸ› Troubleshooting

**No GPU detected**:
```bash
nvidia-smi  # Should show your GPU
sudo reboot  # If drivers just installed
```

**Connection refused**:
- Check security groups (open ports 8000, 9000)
- Verify IPs are correct
- Ensure both shards are running

**Slow generation**:
- Check `nvidia-smi` for GPU utilization  
- Network latency affects cross-shard communication
- Use lower `max_length` for testing

## ğŸŒŸ What's Next?

This is a foundation for **P2P distributed LLMs**. Next steps:
- IPFS model distribution
- DHT-based node discovery  
- Decentralized request routing
- Multi-shard redundancy

## ğŸ“ Files

- `setup_env.sh` - Environment setup
- `setup_shard1.sh` - Start input shard  
- `setup_shard2.sh` - Start output shard
- `setup_client.sh` - Start coordination client
- `shard_server.py` - Shard server implementation
- `distributed_client.py` - Client implementation 